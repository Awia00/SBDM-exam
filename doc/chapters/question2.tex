% !TeX spellcheck = en_GB
\section{Question 2}

\subsection{A)}
\begin{quote}
	\textit{"Consider	the	data	set	from	project	3.	How	much	of	the	work	you	did	in	project	2	to	clean	data	could	be	reused	to	clean	the	data	set	from	project	3?	Explain	your	answer."}
\end{quote}
From a code perspective it would be difficult to reuse the source code of Project 2 to clean the data from Project 3, mostly because we used the serialization framework Avro, which then requires the data to be in a certain format, and outputs data in a specific format to project 2.

\newpar We used streaming to clean the data in Project 2 so in some sense it would be possible to reuse the streaming part, since by lazily streaming the data, it is possible to handle the 80GB of data in Project 3. Then by rewriting the logic of what to remove, label or ignore, the program would eventually complete.

\newpar Referring back to question 1C it should be noted that as soon as we clean data, we can no longer (unless the cleaning only tags, or ignores) assume that the derived data is the same as the primary data. Because of this an approach to backing up the original data or otherwise it should be made very clear that whatever analysis is made on the data will always be done from derived data.

\newpar We choose to not remove or delete any data in the cleaning process but simply ignoring it and allowing the batch computations to decide whether or not to use data. This was done to make it possible for future batch views to use the outliers and missing values for other computations. For example if the operation system was unknown we did not use that WiFi client in the view, but it might be interesting in the future to see how many WiFi clients had unknown OS types. Having information about the what data is not there can be interesting in itself and therefore we did not remove it from the master dataset. 

\subsection{B)}
\begin{quote}
	\textit{"Describe	a	cleaning	process	for	the	data	set	in	project	3.	Describe	the	design	of	a	system	that	implements	this	cleaning	process."}
\end{quote}
A cleaning process over this data could include checking for valid values, such as speed values that are negative. Then checking whether or not each entry falls into one of the two well defined categories, Vehicle-type or Person-type. Another step in the cleaning process would be to check for missing information or information which should not exist for an entity.

\newpar One of specific parts of cleaning the data of Project 3 is the fact that sometimes, when cars have been staying still for too long they are randomly teleported to other parts of the map. If some analysis would be done on location, some cleaning process needs to handle this. This could be done by checking the delta of x and y coordinates compared to the last position of the car. The new value could in case of a teleportation detection, be labelled such that the batch view could handle the entry correctly.

\newpar To create such a cleaning process, I would create a Map-Reduce program such that it can handle the large amounts of data. Then I would create a mapper for each of the different procedures, and checks, which each output to the next mapper in a pipeline fashion. By doing this it is possible for map reduce achieve higher concurrency. A reducer could then be placed at the end, aggregating all the results into two lists of entities, one for vehicles and one for people. At the end the results could be stored on HDFS, ready for batch or streaming analysis.

One could also implement this process as a Hive job, which would have the obvious advantage that Hive itself would split the process into multiple stages of mappers and reducers automatically. Though one disadvantage of this approach is that the data would first have to be imported into Hive tables and then the result would have to be put into another table, or the original data removed.